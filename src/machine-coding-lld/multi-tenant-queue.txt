# Multi-Tenant Fair Message Queue System

## Problem Statement
- Create a producer-consumer architecture for multiple tenants (5 in this example)
- Ensure fairness in message processing across tenants
- Prevent a tenant with a large number of messages (e.g., 100,000) from blocking a tenant with fewer messages (e.g., 1,000)

## Solution Design

We'll implement a round-robin scheduling approach where:
1. Each tenant has its own queue
2. The producer rotates through tenant queues
3. We'll use a configurable batch size to prevent starvation

```java
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * A fair multi-tenant message queue system that ensures messages from all tenants
 * are processed fairly, regardless of queue size differences.
 */
public class MultiTenantFairQueue {
    
    public static void main(String[] args) {
        // Initialize the system with 5 tenants
        MultiTenantMessageSystem system = new MultiTenantMessageSystem(5);
        
        // Start the consumer threads
        system.startConsumers(3);  // 3 consumer threads
        
        // Simulate different message loads for different tenants
        Thread producerThread = new Thread(() -> {
            // Tenant 1 has 100,000 messages
            for (int i = 0; i < 100000; i++) {
                system.addMessage(1, "Tenant 1 Message " + i);
            }
            
            // Tenant 2 has 1,000 messages
            for (int i = 0; i < 1000; i++) {
                system.addMessage(2, "Tenant 2 Message " + i);
            }
            
            // Tenant 3 has 10,000 messages
            for (int i = 0; i < 10000; i++) {
                system.addMessage(3, "Tenant 3 Message " + i);
            }
            
            // Tenant 4 has 500 messages
            for (int i = 0; i < 500; i++) {
                system.addMessage(4, "Tenant 4 Message " + i);
            }
            
            // Tenant 5 has 5,000 messages
            for (int i = 0; i < 5000; i++) {
                system.addMessage(5, "Tenant 5 Message " + i);
            }
        });
        
        producerThread.start();
        
        // Let the system run for some time
        try {
            Thread.sleep(5000);
            system.printStats();
            Thread.sleep(5000);
            system.printStats();
            Thread.sleep(5000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        
        // Shutdown the system
        system.shutdown();
    }
}

/**
 * Message class representing a message in the system.
 */
class Message {
    private final int tenantId;
    private final String content;
    private final long timestamp;
    
    public Message(int tenantId, String content) {
        this.tenantId = tenantId;
        this.content = content;
        this.timestamp = System.currentTimeMillis();
    }
    
    public int getTenantId() {
        return tenantId;
    }
    
    public String getContent() {
        return content;
    }
    
    public long getTimestamp() {
        return timestamp;
    }
    
    @Override
    public String toString() {
        return "Message{" +
                "tenantId=" + tenantId +
                ", content='" + content + '\'' +
                ", timestamp=" + timestamp +
                '}';
    }
}

/**
 * The core system class that manages message queues for multiple tenants.
 */
class MultiTenantMessageSystem {
    private final int numTenants;
    private final Map<Integer, BlockingQueue<Message>> tenantQueues;
    private final ExecutorService consumerPool;
    private final Map<Integer, AtomicInteger> messageProcessedCount;
    private final Map<Integer, AtomicInteger> messageAddedCount;
    private volatile boolean isRunning;
    
    // Configure these parameters based on your requirements
    private static final int BATCH_SIZE = 10;  // Number of messages to process per tenant in one round
    private static final int QUEUE_CAPACITY = 1000;  // Maximum queue size per tenant
    
    public MultiTenantMessageSystem(int numTenants) {
        this.numTenants = numTenants;
        this.tenantQueues = new ConcurrentHashMap<>();
        this.messageProcessedCount = new ConcurrentHashMap<>();
        this.messageAddedCount = new ConcurrentHashMap<>();
        
        // Initialize queues for each tenant
        for (int i = 1; i <= numTenants; i++) {
            tenantQueues.put(i, new LinkedBlockingQueue<>(QUEUE_CAPACITY));
            messageProcessedCount.put(i, new AtomicInteger(0));
            messageAddedCount.put(i, new AtomicInteger(0));
        }
        
        this.consumerPool = Executors.newCachedThreadPool();
        this.isRunning = true;
    }
    
    /**
     * Add a message to a specific tenant's queue.
     * This method will block if the queue is full.
     */
    public void addMessage(int tenantId, String content) {
        if (tenantId < 1 || tenantId > numTenants) {
            throw new IllegalArgumentException("Invalid tenant ID: " + tenantId);
        }
        
        Message message = new Message(tenantId, content);
        try {
            tenantQueues.get(tenantId).put(message);
            messageAddedCount.get(tenantId).incrementAndGet();
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            System.err.println("Interrupted while adding message to queue: " + e.getMessage());
        }
    }
    
    /**
     * Start consumer threads that process messages in a round-robin fashion.
     */
    public void startConsumers(int numConsumers) {
        for (int i = 0; i < numConsumers; i++) {
            consumerPool.submit(this::fairConsumerTask);
        }
    }
    
    /**
     * The fair consumer task that processes messages from all tenants in a round-robin manner.
     */
    private void fairConsumerTask() {
        while (isRunning) {
            // Process each tenant's queue in a round-robin fashion
            for (int tenantId = 1; tenantId <= numTenants && isRunning; tenantId++) {
                BlockingQueue<Message> queue = tenantQueues.get(tenantId);
                
                // Process up to BATCH_SIZE messages from this tenant before moving to the next
                for (int i = 0; i < BATCH_SIZE && isRunning; i++) {
                    Message message = queue.poll();
                    if (message != null) {
                        processMessage(message);
                        messageProcessedCount.get(tenantId).incrementAndGet();
                    } else {
                        // No more messages for this tenant, move to the next one
                        break;
                    }
                }
            }
            
            // If all queues are empty, sleep for a short time to avoid busy-waiting
            boolean allEmpty = true;
            for (int tenantId = 1; tenantId <= numTenants; tenantId++) {
                if (!tenantQueues.get(tenantId).isEmpty()) {
                    allEmpty = false;
                    break;
                }
            }
            
            if (allEmpty) {
                try {
                    Thread.sleep(100);  // Sleep for 100ms if all queues are empty
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                }
            }
        }
    }
    
    /**
     * Process a single message (simulated).
     */
    private void processMessage(Message message) {
        // In a real system, you would process the message here
        // For simulation, we'll just add a small delay
        try {
            // Add a small processing delay (simulate real work)
            Thread.sleep(5);
            
            // Uncomment for debugging:
            // System.out.println("Processed: " + message);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
    }
    
    /**
     * Print statistics about message processing.
     */
    public void printStats() {
        System.out.println("\n===== Message Processing Statistics =====");
        int totalAdded = 0;
        int totalProcessed = 0;
        
        for (int tenantId = 1; tenantId <= numTenants; tenantId++) {
            int added = messageAddedCount.get(tenantId).get();
            int processed = messageProcessedCount.get(tenantId).get();
            int remaining = tenantQueues.get(tenantId).size();
            
            totalAdded += added;
            totalProcessed += processed;
            
            System.out.println("Tenant " + tenantId + ":\t" +
                               "Added: " + added + "\t" +
                               "Processed: " + processed + "\t" +
                               "Remaining: " + remaining);
        }
        
        System.out.println("-----------------------------");
        System.out.println("Total Added: " + totalAdded);
        System.out.println("Total Processed: " + totalProcessed);
        System.out.println("===================================\n");
    }
    
    /**
     * Shutdown the message system.
     */
    public void shutdown() {
        isRunning = false;
        consumerPool.shutdown();
        
        try {
            if (!consumerPool.awaitTermination(5, TimeUnit.SECONDS)) {
                consumerPool.shutdownNow();
            }
        } catch (InterruptedException e) {
            consumerPool.shutdownNow();
            Thread.currentThread().interrupt();
        }
        
        printStats();
    }
}

/**
 * Enhanced version with weighted fair queueing (optional extension)
 */
class WeightedFairMultiTenantSystem {
    // Similar to the basic implementation, but with weights assigned to tenants
    // This could be implemented if certain tenants need priority over others
    // while still maintaining fairness
    
    // For example:
    // Map<Integer, Integer> tenantWeights = new HashMap<>();  // TenantId -> Weight
    // Higher weights get more processing time in each round
    
    // Implementation left as an extension exercise
}
```

## Design Considerations

### Fairness Mechanism
The fairness is achieved through a round-robin scheduling approach where:
1. Each tenant's queue is serviced in a fixed order
2. A configurable batch size limits how many messages can be processed from one tenant before moving to the next
3. This prevents any single tenant from monopolizing the processing resources

### Performance Optimizations
1. **Batch Processing**: The `BATCH_SIZE` parameter allows processing multiple messages from each tenant in one round, improving throughput while maintaining fairness
2. **Separate Queues**: Each tenant has its own queue, allowing for parallel operations
3. **Non-blocking Poll**: Using `poll()` instead of `take()` prevents consumers from blocking on empty queues
4. **Sleep on Empty**: When all queues are empty, the system sleeps briefly to reduce CPU usage

### Scalability
1. The system uses thread pools that can be scaled based on load
2. The number of consumers can be adjusted based on processing requirements
3. Queue capacity can be configured based on memory constraints

### Monitoring
The implementation includes statistics tracking to monitor:
1. Number of messages added per tenant
2. Number of messages processed per tenant
3. Number of messages remaining in each queue

### Extensions
1. **Priority Levels**: Could add priority levels within tenant queues
2. **Weighted Fair Queueing**: Could assign weights to tenants for proportional fairness
3. **Dynamic Batch Sizing**: Could adjust batch size based on queue lengths
4. **Persistent Queues**: Could add persistence for reliability

## Usage Example

The `main` method demonstrates how to use the system with different loads for different tenants. The example creates:
- Tenant 1: 100,000 messages
- Tenant 2: 1,000 messages
- Tenant 3: 10,000 messages
- Tenant 4: 500 messages
- Tenant 5: 5,000 messages

When you run this example, you'll see that messages from all tenants get processed fairly, with Tenant 2 and Tenant 4 (with fewer messages) getting their messages processed without having to wait for Tenant 1's 100,000 messages to complete.