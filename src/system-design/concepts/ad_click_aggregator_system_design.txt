# System Design: Ad Click Aggregator

## 1. Introduction
    - **Purpose of the system**: To accurately and efficiently collect, process, aggregate, and serve ad click data. This system provides crucial insights for advertisers, publishers, and the ad platform itself, enabling campaign performance tracking, billing, fraud detection, and optimization of ad delivery.
    - **Goals and Scope**:
        - **Goals**:
            - Provide a reliable and scalable platform for ingesting high volumes of ad click events.
            - Process and aggregate click data in near real-time.
            - Offer flexible querying capabilities for various stakeholders.
            - Ensure data accuracy and completeness.
            - Support future extensions like advanced fraud detection and A/B testing analytics.
        - **Scope (V1)**:
            - Ingestion of click events with essential metadata.
            - Basic data validation.
            - Aggregation of clicks by common dimensions (ad ID, campaign ID, timestamp, geo).
            - API for retrieving aggregated counts.
            - Deduplication of clicks within a short time window.
        - **Out of Scope (V1)**:
            - Sophisticated real-time fraud detection (basic filtering might be included).
            - Impression tracking and attribution (focus is on clicks).
            - User profile building or advanced behavioral analysis.
            - Integration with billing systems (system will provide data for billing).

## 2. Requirements Clarification
    ### 2.1. Functional Requirements
        - **Click Ingestion**: The system must provide highly available endpoints (e.g., HTTP GET pixel, HTTP POST with payload) to receive ad click events from various sources (web browsers, mobile apps). Each click event should contain details like ad ID, campaign ID, user identifiers (if available), timestamp, IP address, user agent, etc.
        - **Data Validation**: Perform basic validation on incoming click data (e.g., check for required fields, basic format correctness). Invalid or malformed data should be logged or sent to a dead-letter queue.
        - **Real-time / Near Real-time Processing**: Clicks should be processed and made available for aggregation within seconds to a few minutes of occurrence.
        - **Aggregation**: The system must aggregate click counts based on multiple dimensions and time windows.
            - **Dimensions**: Ad ID, Campaign ID, Advertiser ID, Publisher ID, Geo-location (Country, Region, City), Device Type, Operating System, Time (Minute, Hour, Day).
            - **Metrics**: Total Clicks, Unique Clicks (based on user ID or a combination of IP + User Agent if user ID is not available).
        - **Reporting**: Provide APIs to query aggregated click data. Queries should support filtering by dimensions and time ranges. Dashboards for visualization are a plus but APIs are primary.
        - **Uniqueness/Deduplication**: The system should attempt to identify and filter out duplicate clicks generated by accidental multiple clicks from the same user within a short time frame (e.g., within 5-10 seconds for the same user on the same ad).
        - **Scalability**: The system must be designed to handle a significant and growing volume of click events (e.g., millions to billions per day). All components should be horizontally scalable.
        - **Extensibility**: The architecture should allow for easy addition of new aggregation dimensions, metrics, or data sources in the future.
    ### 2.2. Non-Functional Requirements
        - **High Availability**: The system, especially the click ingestion part, should have at least 99.99% uptime. Aggregation and reporting can have slightly lower HA, e.g., 99.9%.
        - **High Throughput**:
            - **Ingestion**: Support peak ingestion rates of 100,000s clicks per second (CPS). Average could be 10,000s CPS.
            - **Processing**: Process data at a rate matching or exceeding ingestion.
        - **Low Latency**:
            - **Ingestion**: Acknowledgement of click receipt within < 50ms.
            - **End-to-End Aggregation Latency**: Aggregated data available for querying within 1-5 minutes of the click event.
            - **Query Latency**: P95 for typical reporting queries < 1 second.
        - **Durability**: No loss of valid click data once accepted by the ingestion service. Data should be persisted reliably.
        - **Consistency**:
            - For raw click data: Strong consistency upon ingestion.
            - For aggregated counts: Eventual consistency is acceptable for most reporting (e.g., dashboards). For critical counts (e.g., feeding into billing), higher consistency or reconciliation mechanisms might be needed.
        - **Scalability**: All components (ingestion, processing, storage, reporting) must be able to scale horizontally to handle increasing load.
        - **Fault Tolerance**: The system should be resilient to individual component failures without data loss or significant service disruption. Automatic failover mechanisms should be in place.
        - **Security**:
            - Secure communication (HTTPS) for click ingestion and API access.
            - Protection against common web vulnerabilities (e.g., DDoS at the edge).
            - Proper authentication and authorization for reporting APIs.
            - Data privacy considerations for user-related data.
        - **Cost-Effectiveness**: Optimize infrastructure and operational costs by choosing appropriate technologies and scaling resources efficiently.
        - **Maintainability**: The system should be easy to monitor, debug, and update. Clear logging and metrics are essential.
    ### 2.3. Extended Requirements (Out of Scope for V1 but good to consider for future)
        - **Advanced Fraud Detection**: Implement sophisticated algorithms (rule-based and ML-based) to identify and filter out invalid/bot clicks in real-time or batch.
        - **A/B Testing Support**: Facilitate aggregation and comparison of click data for different versions of ads or landing pages.
        - **Real-time Anomaly Detection**: Monitor click patterns and alert on significant deviations or suspicious activities.
        - **Direct Integration with Billing Systems**: Provide verified and auditable click counts directly to billing systems.
        - **User Attribution**: More complex models to attribute clicks to various touchpoints in a user's journey.
        - **Click Path Analysis**: Understanding sequences of clicks.

## 3. Capacity Estimation & System Constraints

Let's assume the following for our ad platform:

    - **Ad Impressions per day**: 10 billion impressions
    - **Average Click-Through Rate (CTR)**: 0.5% (This can vary wildly, but 0.5% is a reasonable starting point for estimation)
    - **Total Clicks per day**: 10,000,000,000 impressions * 0.005 = 50,000,000 (50 million) clicks/day

    - **Peak Traffic Factor**: Peak traffic can be 3-5x average. Let's use 4x.
    - **Average Clicks Per Second (CPS)**:
        - 50,000,000 clicks / (24 hours * 60 minutes * 60 seconds)
        - 50,000,000 / 86,400 seconds ~= 578 CPS (average)
    - **Peak Clicks Per Second (CPS)**:
        - 578 CPS * 4 (peak factor) ~= 2,312 CPS (peak)
        - Let's round this up for design considerations: **Peak Ingestion Rate: ~2,500 - 3,000 CPS**.
        - For future growth and unexpected spikes, the system should be designed to handle **~10,000 CPS**, and be able to scale beyond.

    - **Data Size per Click Event**:
        - Assuming a JSON payload for a click event.
        - Fields: `click_id` (UUID, 36 bytes), `timestamp` (long, 8 bytes), `ad_id` (UUID, 36 bytes), `campaign_id` (UUID, 36 bytes), `user_id` (UUID, 36 bytes), `ip_address` (string, ~15-40 bytes), `user_agent` (string, ~100-200 bytes), `referrer_url` (string, ~50-200 bytes), `geo_data` (string, ~50 bytes), `device_type` (string, ~20 bytes), `impression_id` (UUID, 36 bytes).
        - Other metadata, JSON overhead.
        - Estimated average size per click event: **~500 Bytes - 1 KB**. Let's use 1 KB for easier calculation.

    - **Total Daily Ingestion Data Volume**:
        - 50,000,000 clicks/day * 1 KB/click = 50,000,000 KB/day = 50 GB/day.

    - **Total Monthly Ingestion Data Volume**:
        - 50 GB/day * 30 days = 1500 GB/month = 1.5 TB/month.

    - **Total Yearly Ingestion Data Volume (Raw Clicks)**:
        - 1.5 TB/month * 12 months = 18 TB/year.
        - If raw click data is stored for a long period (e.g., 1 year for audit/reprocessing), this is the storage needed.

    - **Storage for Aggregated Data**:
        - This depends heavily on the number of dimensions, granularity of aggregation, and retention period.
        - Example: Aggregating by (Ad ID, Campaign ID, Geo (Country), Device Type, Hour).
            - Assume 100,000 Ad IDs, 10,000 Campaign IDs, 200 Geos, 5 Device Types.
            - Number of potential combinations is huge, but only active combinations will store data.
            - Let's say 1 million active unique dimension combinations per hour.
            - Data per aggregated record: `ad_id, campaign_id, geo, device, hour_timestamp, click_count, unique_user_count` (~100-200 bytes).
            - 1,000,000 records/hour * 200 bytes/record * 24 hours/day = 4.8 GB/day for this specific aggregation.
            - If multiple aggregation granularities (minute, hour, day) and dimension sets are stored, this can grow significantly.
            - For V1, let's estimate aggregated data storage to be **~10-20% of raw data storage if raw data is heavily downsampled**, or potentially comparable if many fine-grained aggregations are kept long-term.
            - Let's assume **~2-4 TB/year for aggregated data** with reasonable retention (e.g., hourly for 3 months, daily for 1 year).

    - **Read QPS for Reporting API**:
        - This can vary. Assume a few hundred concurrent analysts/systems querying data.
        - Typical queries might fetch time series data for a specific campaign or ad.
        - Peak Read QPS: **~500-1000 QPS** on the reporting database/API layer.
        - Dashboard auto-refreshes can contribute significantly.

    - **Write QPS for Aggregated Data Store**:
        - Stream processor outputting aggregated counts.
        - If aggregating every minute: 50,000,000 clicks / (24*60 minutes) = ~34,722 clicks per minute on average.
        - These are aggregated into fewer records. If 100,000s of distinct dimension combinations are updated per minute/5-minutes, the write QPS to the aggregated store could be in the range of **1,000s to 10,000s of writes per second** (batch updates are likely).

    - **Latency Requirements (reiteration from NFRs in context of scale)**:
        - **Click Ingestion API**: p99 latency < 50ms.
        - **Data Visibility (Event to Aggregated Query)**: p95 < 1 minute (near real-time). Some less critical aggregations can be < 5-15 minutes.
        - **Reporting API Query Latency**:
            - Simple queries (e.g., clicks for one ad today): p95 < 500ms.
            - Complex queries (e.g., trends over a month across many ads): p95 < 5 seconds.

    - **System Constraints**:
        - **Global Deployment**: Ad clicks can originate globally. Ingestion points should be geographically distributed to reduce latency.
        - **Data Consistency**: Eventual consistency for most aggregated views is acceptable. For billing-related counts, a reconciliation process or more consistent path might be needed (possibly batch-verified).
        - **Cost**: The solution must be cost-effective at scale. Serverless options, efficient data storage, and auto-scaling are important.
        - **Schema Evolution**: The system should gracefully handle changes in click event schema or aggregation requirements.

## 4. High-Level Design

The Ad Click Aggregator system will be designed as a distributed, scalable, and fault-tolerant pipeline. It will consist of several key stages: ingestion, queuing, processing/aggregation, storage, and reporting.

    - **System Architecture Diagram (Conceptual)**:
        (Imagine a diagram here. A textual description follows)

        Clients (Browsers, Mobile Apps)
              |
              v
        [Load Balancer (e.g., AWS ALB/NLB, Nginx)]
              |
              v
        [**1. Click Ingestion Service** (Stateless, Horizontally Scalable, e.g., EC2 Auto Scaling Group, Kubernetes Deployment)]
        (Validates, enriches minimally, forwards to Message Queue)
              |
              v
        [**2. Message Queue** (e.g., Apache Kafka, AWS Kinesis, Google Pub/Sub)]
        (Decouples ingestion from processing, provides buffering, durability)
              |
              v
        [**3. Stream Processing Engine / Aggregation Service** (e.g., Apache Flink, Spark Streaming, Kafka Streams, AWS Kinesis Data Analytics)]
        (Consumes from Message Queue, performs deduplication, enrichment, aggregation over time windows)
              |----------------------------------------|
              v                                        v
        [**4.a. Raw Click Storage (Optional)**]     [**4.b. Aggregated Data Storage**]
        (e.g., S3 Data Lake, HDFS)                (e.g., Time-series DB like TimescaleDB/InfluxDB,
        (For archival, reprocessing, batch analytics)  Key-value/Wide-column like Cassandra for counters,
                                                       OLAP DB/Data Warehouse like ClickHouse/Druid/Snowflake)
                                                         |
                                                         v
                                        [**5. Reporting & Query API** (e.g., REST/GraphQL API)]
                                        (Serves aggregated data to dashboards, analysts, other systems)
                                                         |
                                                         v
                                        [Clients (Dashboards, BI Tools, Internal Services)]

        --- Common Services ---
        - **Metadata Store** (e.g., PostgreSQL, MySQL): Stores ad/campaign metadata. Used by Ingestion & Processing.
        - **Monitoring & Alerting System** (e.g., Prometheus, Grafana, CloudWatch): Collects metrics from all components.
        - **Service Discovery** (e.g., Consul, etcd, Kubernetes DNS)
        - **Configuration Management**

    - **Core Components Overview**:
        - **Click Ingestion Service**:
            - **Responsibility**: Receives raw click events from clients (browsers, mobile apps) via highly available endpoints (e.g., HTTP GET pixel tracker, HTTP POST).
            - **Functions**: Performs initial lightweight validation (e.g., presence of essential parameters), adds server-side timestamp, potentially enriches with geo-location data based on IP, and forwards the click event to the Message Queue.
            - **Characteristics**: Stateless, horizontally scalable, low latency.
        - **Message Queue**:
            - **Responsibility**: Acts as a durable, scalable buffer between the Ingestion Service and the Stream Processing Engine.
            - **Functions**: Decouples producers from consumers, handles backpressure, allows for replayability (for some systems like Kafka), and enables multiple consumers for different purposes (e.g., real-time aggregation, batch processing, fraud detection).
            - **Characteristics**: High throughput, persistent, partitioned for scalability.
        - **Stream Processing Engine / Aggregation Service**:
            - **Responsibility**: Consumes click events from the Message Queue in real-time or near real-time.
            - **Functions**:
                - **Deduplication**: Identifies and filters out duplicate clicks based on defined criteria (e.g., same user, same ad, within a short time window).
                - **Enrichment**: Augments click data with additional information from the Metadata Store (e.g., campaign details, advertiser info).
                - **Aggregation**: Performs stateful aggregations of click counts over various time windows (e.g., 1-minute, 5-minute, hourly) and dimensions (ad ID, campaign ID, geo, device, etc.).
                - **Output**: Writes aggregated results to the Aggregated Data Storage and potentially raw/enriched events to Raw Click Storage.
            - **Characteristics**: Scalable, fault-tolerant, supports stateful operations and windowing.
        - **Data Storage**:
            - **Raw Click Storage (Optional but Recommended)**: Stores raw or minimally processed click events for archival, auditing, reprocessing in case of errors, or for batch analytics and ML model training. (e.g., S3, HDFS).
            - **Aggregated Data Storage**: Stores the aggregated click counts. The choice of database depends on query patterns and latency requirements.
                - Time-series databases (e.g., InfluxDB, TimescaleDB, Prometheus) are good for time-based rollups and queries.
                - Key-value/Wide-column stores (e.g., Cassandra, Redis, HBase) can be used for fast lookups of pre-aggregated counters.
                - OLAP databases / Data Warehouses (e.g., ClickHouse, Apache Druid, Snowflake, BigQuery, Redshift) are suitable for complex analytical queries on large aggregated datasets.
            - **Metadata Store**: Stores relatively static data like ad definitions, campaign information, user segments, etc. (e.g., PostgreSQL, MySQL).
        - **Reporting & Query API**:
            - **Responsibility**: Provides an interface (e.g., REST, GraphQL) for clients (dashboards, BI tools, internal systems) to query the aggregated click data.
            - **Functions**: Translates API requests into queries for the Aggregated Data Storage, formats results, and handles authentication/authorization. May include caching for frequently accessed data.
            - **Characteristics**: Scalable, secure, provides flexible query options.
        - **Monitoring & Alerting**:
            - **Responsibility**: Collects metrics, logs, and traces from all components of the system.
            - **Functions**: Provides dashboards for system health, performance monitoring, and business KPIs. Triggers alerts for anomalies, errors, or SLA breaches.
            - **Characteristics**: Comprehensive, real-time.

## 5. Detailed Component Design

    ### 5.1. Click Ingestion
        - **API Design**:
            - **Primary Method: HTTP GET with 1x1 Pixel Tracking**:
                - Endpoint: `GET /track/click?param1=val1&param2=val2...`
                - Client (browser) requests a transparent 1x1 pixel image. Query parameters carry click data.
                - Low overhead, widely compatible, good for web browser clicks.
                - Parameters: `ad_id`, `campaign_id`, `user_id` (if available), `impression_id`, `redirect_url` (URL to redirect user to after logging click), `cb` (cache buster).
                - Server responds with a `204 No Content` or the 1x1 pixel image and then performs an HTTP redirect to the `redirect_url` if provided.
            - **Alternative Method: HTTP POST with JSON Payload**:
                - Endpoint: `POST /track/click_event`
                - Body: JSON object containing detailed click information.
                - Suitable for server-to-server tracking or mobile app clicks where more data can be sent reliably.
                - Schema: Matches the Click Event Schema defined in Section 6.1.
            - **Geo-distributed Endpoints**: Use services like AWS Global Accelerator or Cloudflare to route users to the nearest ingestion endpoint, reducing latency.
        - **Data Format**:
            - For GET: Query string parameters.
            - For POST: JSON is preferred for its flexibility and wide support. Protobuf could be considered for performance-critical server-to-server communication to reduce payload size and serialization/deserialization overhead.
        - **Initial Validation**:
            - **Mandatory fields**: Check for presence of critical fields like `ad_id`, `campaign_id`, `timestamp`.
            - **Data types/formats**: Basic validation (e.g., timestamp is a valid long, IDs are in expected format).
            - **Request size limits**: Protect against overly large payloads.
            - Malformed or invalid requests are rejected with an appropriate HTTP error code (e.g., 400 Bad Request) and logged.
        - **Enrichment (Lightweight)**:
            - **Server-side Timestamp**: Add a server-received timestamp to distinguish from client-reported timestamp and help with ordering/latency analysis.
            - **IP-based Geo-lookup**: If client doesn't provide geo-location, perform a lookup based on the source IP address (using a local MaxMind GeoIP database or a fast lookup service). This should be done asynchronously or with a very low-latency local cache to not impact ingestion latency.
        - **Load Balancing**:
            - Use multiple layers of load balancers (e.g., Edge LBs, Regional LBs, Application LBs) to distribute traffic across a fleet of stateless ingestion service instances.
            - Techniques: Round Robin, Least Connections.
        - **Scalability & Resilience**:
            - Service instances should be stateless.
            - Deployed in an auto-scaling group (e.g., Kubernetes HPA, EC2 Auto Scaling) based on CPU utilization or request count.
            - Health checks for instances.
        - **Security**:
            - HTTPS for all endpoints.
            - Rate limiting per IP/API key to prevent abuse.
            - Consider WAF (Web Application Firewall) at the edge.
    ### 5.2. Message Queue
        - **Choice of technology**:
            - **Apache Kafka**: Strong choice due to high throughput, scalability, durability, replayability, and rich ecosystem. Good for large-scale event streaming.
            - **AWS Kinesis Data Streams**: Fully managed, good integration with AWS ecosystem, auto-scaling. Simpler to manage than self-hosted Kafka.
            - **Google Cloud Pub/Sub**: Similar to Kinesis, fully managed, global by default.
            - **Decision**: **Apache Kafka** is often preferred for its flexibility and performance, especially if not fully locked into one cloud provider or if advanced stream processing features are heavily used. If fully on AWS and preferring managed services, Kinesis is excellent. Let's assume Kafka for this design for its widespread use.
        - **Topics and Partitions Strategy**:
            - **Topic**: A single topic for all click events, e.g., `ad_clicks`.
            - **Partitions**: Partition the topic to enable parallel consumption and increase throughput. Number of partitions based on expected throughput and consumer parallelism (e.g., 64, 128, or more).
            - **Partition Key**:
                - `ad_id` or `campaign_id`: Ensures clicks for the same ad/campaign go to the same partition, which can be useful for certain types of stateful processing or locality if consumers are optimized for it. However, can lead to hot partitions if some ads/campaigns are extremely popular.
                - `user_id` (if available and high cardinality): Good for user-centric processing, but similar hot partition risk.
                - `click_id` (random UUID) or Round Robin: Provides the best distribution of load across partitions, avoiding hot spots. This is generally preferred for initial ingestion unless specific ordering by another key is critical for an immediate downstream consumer.
                - **Recommendation**: Use `click_id` or a random key for even distribution into the main `ad_clicks` topic. Further re-partitioning can be done by downstream stream processors if needed.
        - **Data Serialization**:
            - **Apache Avro or Protobuf**: Recommended for Kafka messages. They provide schema evolution, compact binary format (reducing network bandwidth and storage), and efficient serialization/deserialization compared to JSON.
            - Schema Registry (e.g., Confluent Schema Registry) should be used with Avro/Protobuf to manage schemas.
        - **Durability & Replication**:
            - Configure Kafka with a replication factor of 3 (across different availability zones).
            - Ensure `acks=all` for producers to guarantee messages are written to all ISRs (In-Sync Replicas) before acknowledgement.
        - **Retention Policy**:
            - Define a retention period for data in Kafka topics (e.g., 3-7 days). This allows for reprocessing if needed and acts as a buffer. Longer-term storage should be in Raw Click Storage (e.g., S3).
        - **Producers**: The Click Ingestion Service instances are Kafka producers.
        - **Consumers**: The Stream Processing Engine instances are Kafka consumers.
    ### 5.3. Stream Processing & Aggregation
        - **Choice of technology**:
            - **Apache Flink**: Powerful stream processing framework with excellent support for event time processing, state management, windowing, and exactly-once semantics. Good for complex aggregations.
            - **Apache Spark Streaming**: Micro-batch based, mature, good for both streaming and batch workloads.
            - **Kafka Streams**: A library for building streaming applications directly on Kafka. Simpler for use cases tightly coupled with Kafka, but might be less feature-rich for very complex stateful operations compared to Flink.
            - **AWS Kinesis Data Analytics** (if using Kinesis Streams): Managed Flink or SQL-based stream processing.
            - **Decision**: **Apache Flink** is a strong candidate for its robust streaming capabilities, especially for handling out-of-order events (event time processing) and complex stateful aggregations.
        - **Core Processing Logic**:
            1. **Consume from Kafka**: Flink job consumes click events from the `ad_clicks` Kafka topic.
            2. **Deserialization & Validation**: Deserialize messages (e.g., Avro/Protobuf) and perform any deeper validation if needed. Malformed messages to DLQ.
            3. **Deduplication**:
                - Based on `click_id` if globally unique and sent by client.
                - More commonly, based on a composite key like (`user_id`, `ad_id`, `impression_id`) within a short time window (e.g., 5-30 seconds).
                - Flink's keyed state can be used to store recent click identifiers for deduplication. Bloom filters can optimize this for very high cardinality keys.
            4. **Enrichment**:
                - Join click events with metadata from a side input or a lookup table (e.g., campaign details, ad creative info, advertiser info from the Metadata Store). Use Flink's broadcast state or asynchronous I/O for external lookups with caching.
            5. **Keying / Grouping**: Key events by dimensions required for aggregation (e.g., `(ad_id, campaign_id, geo, device_type, time_bucket)`).
            6. **Windowing**:
                - **Tumbling Windows**: For fixed, non-overlapping time intervals (e.g., every 1 minute, 5 minutes, 1 hour). `TumblingEventTimeWindows.of(Time.minutes(5))`
                - **Sliding Windows**: For overlapping intervals (e.g., count clicks in the last 1 hour, updated every 5 minutes).
                - **Session Windows**: To group events by user activity sessions (less common for basic click counting but useful for user behavior analysis).
                - **Event Time Processing**: Crucial to handle late-arriving data correctly. Configure watermarks to track the progress of event time.
            7. **Aggregation Functions**:
                - Count total clicks.
                - Count unique users (using HyperLogLog or similar probabilistic data structure for memory efficiency if exact counts are not strictly needed for all dimensions, or exact counts if state size permits).
            8. **Output**: Write aggregated results to the Aggregated Data Storage.
        - **State Management**:
            - Flink manages operator state. Use a robust state backend (e.g., RocksDB on local disk, replicated to HDFS/S3 for fault tolerance).
            - Checkpointing should be enabled for fault tolerance, allowing recovery from the last successful checkpoint.
        - **Handling Late-Arriving Data**:
            - Use event time processing and watermarks.
            - Configure an `allowedLateness` for windows to include late events.
            - Late events beyond `allowedLateness` can be sent to a side output for separate processing or logging.
        - **Scalability & Parallelism**:
            - Flink jobs can be scaled by adjusting the parallelism of operators.
            - Ensure Kafka topic partitions and Flink parallelism are aligned.
        - **Exactly-Once Semantics (EOS)**:
            - Aim for EOS from Kafka to Flink to Aggregated Data Store. Flink supports this with transactional Kafka producers and specific connectors for data sinks. This prevents data loss or duplication during failures.
    ### 5.4. Data Storage
        #### 5.4.1. Raw Click Storage (Optional, for reprocessing/auditing)
            - **Purpose**: Store raw or lightly processed click events for long-term archival, batch analytics, ML model training (e.g., for fraud detection), and reprocessing in case of pipeline errors or logic changes.
            - **Choice of technology**:
                - **Cloud Object Storage (e.g., AWS S3, Google Cloud Storage, Azure Blob Storage)**: Highly scalable, durable, cost-effective. Preferred choice.
                - **HDFS (Hadoop Distributed File System)**: If an existing Hadoop ecosystem is in place.
            - **Data Format**:
                - **Apache Parquet or ORC**: Columnar formats, excellent for analytical queries, good compression. Preferred over raw JSON/Avro for storage efficiency and query performance by tools like Spark, Presto, Athena.
            - **Organization**:
                - Partition data by date (year, month, day, hour) for efficient querying and data lifecycle management. E.g., `s3://ad-clicks-raw/year=YYYY/month=MM/day=DD/hour=HH/`.
            - **Data Ingestion**: Stream processor (Flink) can write to this storage in batches, or a separate batch job can consume from Kafka and write.
            - **Retention**: Long-term (e.g., 1-3 years or as per compliance). Implement lifecycle policies to move older data to cheaper storage tiers (e.g., S3 Glacier).

        #### 5.4.2. Aggregated Data Storage
            - **Purpose**: Store the results of the stream processing (aggregated click counts by various dimensions and time windows) for fast querying by the Reporting API.
            - **Choice of technology (depends on query patterns, latency, scale, consistency needs)**:
                - **Option A: Time-Series Database (TSDB)**
                    - **Examples**: InfluxDB, TimescaleDB (PostgreSQL extension), Prometheus (more for metrics monitoring but can be used).
                    - **Pros**: Optimized for time-series data, efficient time-based queries, rollups, downsampling. Good for dashboards showing trends.
                    - **Cons**: May be less flexible for arbitrary dimensional querying compared to OLAP DBs. Scalability can vary.
                - **Option B: OLAP Database / Data Warehouse**
                    - **Examples**: Apache Druid, ClickHouse, Snowflake, Google BigQuery, AWS Redshift.
                    - **Pros**: Excellent for fast analytical queries on large datasets, slicing and dicing by many dimensions. Highly scalable. Druid and ClickHouse are known for low-latency queries.
                    - **Cons**: Can be more complex to set up and manage (though managed services simplify this). Some might have higher ingestion latency than specialized TSDBs for very fine-grained real-time updates.
                - **Option C: Key-Value/Wide-Column Store (for pre-aggregated counters)**
                    - **Examples**: Apache Cassandra, HBase, Redis (for very hot counters with shorter retention).
                    - **Pros**: Extremely high write throughput, good for updating counters. Can serve simple lookups very fast.
                    - **Cons**: Less flexible for analytical queries across many dimensions or arbitrary time ranges. Data modeling can be complex to support various query patterns. Often used in conjunction with another system for broader analytics.
                - **Decision for V1**:
                    - **Apache Druid or ClickHouse** are strong contenders for their real-time ingestion capabilities and fast OLAP query performance. They can handle high cardinality dimensions and large data volumes.
                    - **TimescaleDB** could be a simpler starting point if the team has strong PostgreSQL expertise and the scale/complexity doesn't immediately warrant a separate large OLAP cluster.
                    - Let's lean towards **Apache Druid** for its real-time ingestion, pre-aggregation capabilities (roll-ups at ingestion), and excellent query performance for the types of queries an ad aggregator needs.
            - **Data Model for Aggregated Store (e.g., in Druid)**:
                - **Datasource (Table)**: e.g., `ad_clicks_aggregated`
                - **Timestamp Column**: `__time` (event time, truncated to window boundary, e.g., minute, hour).
                - **Dimensions**: `ad_id`, `campaign_id`, `advertiser_id`, `publisher_id`, `geo_country`, `geo_city`, `device_type`, `os`, etc. (as strings, dictionary-encoded by Druid).
                - **Metrics**: `click_count` (longSum), `unique_user_count_hll` (hyperUnique aggregator on user IDs).
                - **Rollup**: Druid can perform rollups at ingestion time, meaning rows with identical dimensions and timestamp (within a segment's time chunk) are pre-aggregated.
            - **Ingestion**: Flink job writes aggregated data to Druid via its Kafka ingestion spec or a dedicated Druid connector.
            - **Retention**: Varies by granularity (e.g., minute-level for 7 days, hourly for 3 months, daily for 1-2 years). Druid supports tiered storage and automatic data retention rules.

        #### 5.4.3. Metadata Storage
            - **Purpose**: Store metadata about ads, campaigns, advertisers, publishers, etc. This data is used for enrichment in the stream processing stage and potentially for filtering/dimension lookup in reporting.
            - **Choice of technology**:
                - **Relational Database (e.g., PostgreSQL, MySQL, AWS Aurora)**: Suitable for structured data, transactional consistency, and relational queries.
            - **Data Model**: Standard relational tables for `campaigns`, `ads`, `advertisers`, etc., with appropriate relationships.
            - **Access Pattern**:
                - Stream processor (Flink) might need to access this data frequently for enrichment. Caching strategies (e.g., Flink's broadcast state for smaller tables, or an external cache like Redis populated from the RDBMS) are essential to avoid overwhelming the RDBMS.
                - Asynchronous I/O with a local cache in Flink is a common pattern.
    ### 5.5. Reporting & Querying
        - **API Design**:
            - **RESTful API or GraphQL API**:
                - `GET /reports/clicks/timeseries`
                    - Query Params: `start_time`, `end_time`, `granularity` (minute, hour, day), `dimensions` (comma-separated list: ad_id, campaign_id, etc.), `filters` (e.g., `ad_id=123,campaign_id=456`), `metrics` (click_count, unique_user_count).
                - `GET /reports/clicks/topN`
                    - Query Params: `start_time`, `end_time`, `dimension` (e.g., ad_id), `metric` (click_count), `top_n` (e.g., 10), `filters`.
            - Response format: JSON.
        - **API Gateway**: Use an API Gateway (e.g., AWS API Gateway, Apigee, Kong) for request routing, rate limiting, authentication, caching, and request/response transformation.
        - **Query Service Layer**:
            - A dedicated service that sits between the API Gateway and the Aggregated Data Store.
            - **Responsibilities**:
                - Authenticate/authorize requests.
                - Parse and validate API requests.
                - Translate API requests into native queries for the chosen Aggregated Data Store (e.g., Druid SQL or native JSON queries).
                - Execute queries.
                - Format results.
                - Implement caching for common queries (e.g., using Redis or Memcached).
        - **Dashboarding Tools Integration**:
            - The Aggregated Data Store (e.g., Druid) should support direct connections from BI/dashboarding tools like Apache Superset, Grafana, Tableau, Looker via JDBC/ODBC or native connectors.
        - **Query Optimization**:
            - Ensure the Aggregated Data Store is properly indexed/partitioned.
            - Design schemas in the aggregated store to match common query patterns.
            - Use materialized views or pre-aggregated rollups where appropriate.
            - Implement query timeouts and resource limits to prevent runaway queries.
        - **Security**:
            - Authentication (e.g., OAuth2, API Keys) for all API endpoints.
            - Authorization (role-based access control) to restrict access to data based on user roles/permissions.
    ### 5.6. Fraud Detection (Brief for V1, more in Extended)
        - **V1 Approach (Basic)**:
            - **Simple Rules in Stream Processor (Flink)**:
                - **IP Blacklisting**: Check against a known blacklist of fraudulent IPs (can be updated periodically).
                - **User-Agent Filtering**: Filter out known bot user-agents.
                - **Click Velocity/Frequency Capping**: Basic checks like "too many clicks from the same IP/user on the same ad in a very short period."
            - These basic checks can mark a click as "suspicious" or filter it out before aggregation. Filtered clicks can be sent to a separate stream/storage for analysis.
        - **Data for Advanced Fraud Detection**:
            - Raw click stream (from Kafka or Raw Click Storage) is essential for training and running more sophisticated ML-based fraud detection models.
            - These models would typically run as a separate batch or near real-time process. Their output (e.g., a fraud score per click, or a list of fraudulent click IDs) can then be used to:
                - Exclude fraudulent clicks from billing aggregations.
                - Provide insights into fraud patterns.
        - **Integration**:
            - The stream processor can be enhanced to look up fraud scores from an external service or join with a stream of fraud decisions.

## 6. Data Models / Database Schema
    ### 6.1. Click Event Schema (e.g., for Kafka messages, Raw Storage)
        - This schema represents the data captured for each click event.
        - **Format**: JSON or Avro/Protobuf (Avro/Protobuf preferred for Kafka for schema evolution and efficiency).
        - **Fields**:
            - `event_id` (UUID, string): Unique identifier for the click event. Generated by ingestion service or client.
            - `timestamp_ms` (long): Epoch milliseconds when the click occurred (client-side, if available).
            - `server_timestamp_ms` (long): Epoch milliseconds when the event was received by the ingestion service.
            - `ad_id` (string/int): Identifier for the specific ad creative.
            - `campaign_id` (string/int): Identifier for the advertising campaign.
            - `advertiser_id` (string/int): Identifier for the advertiser.
            - `publisher_id` (string/int): Identifier for the publisher/website where the ad was shown.
            - `user_id` (string, optional): Identifier for the user (e.g., cookie ID, device ID, internal user ID). Hashed or anonymized if PII.
            - `ip_address` (string): IP address of the user. Used for geo-lookup and fraud detection. May be anonymized/truncated after initial processing for privacy.
            - `user_agent` (string): User-Agent string from the client's browser/device.
            - `click_url` (string, optional): The destination URL the user was redirected to.
            - `impression_id` (string, optional): ID of the impression that led to this click (for click-through rate calculation).
            - `geo_country` (string, enriched): ISO country code.
            - `geo_region` (string, enriched, optional): Region/state.
            - `geo_city` (string, enriched, optional): City.
            - `device_type` (string, enriched, optional): e.g., "desktop", "mobile", "tablet".
            - `os` (string, enriched, optional): Operating system, e.g., "iOS", "Android", "Windows".
            - `browser` (string, enriched, optional): Browser name, e.g., "Chrome", "Firefox".
            - `is_fraudulent` (boolean, enriched, optional): Flag set by fraud detection. Default false.
            - `fraud_score` (float, enriched, optional): Score from fraud detection model.
            - `additional_params` (map<string, string>, optional): For any other custom tracking parameters.

    ### 6.2. Aggregated Data Schema (e.g., for Druid, ClickHouse)
        - This schema represents the structure of the aggregated data stored for reporting.
        - The exact schema depends on the chosen OLAP database, but here's a general structure, assuming Apache Druid.

        #### 6.2.1. Example: Hourly Aggregation Table (`ad_clicks_hourly`)
            - **Timestamp Column**:
                - `__time` (timestamp): Druid's primary time column, representing the start of the hour window (e.g., `2023-10-26T10:00:00Z`).
            - **Dimension Columns** (indexed for filtering and grouping):
                - `ad_id` (string)
                - `campaign_id` (string)
                - `advertiser_id` (string)
                - `publisher_id` (string)
                - `geo_country` (string)
                - `geo_city` (string, optional)
                - `device_type` (string)
                - `os` (string, optional)
                - `browser` (string, optional)
                - `time_granularity` (string, e.g., "hour", can be implicit if table is specific to a granularity)
            - **Metric Columns** (values to be aggregated):
                - `click_count` (longSum aggregator in Druid): Total number of clicks.
                - `unique_users_hll` (hyperUnique aggregator in Druid on `user_id`): Approximate count of unique users.
                - `approx_unique_ips_hll` (hyperUnique aggregator in Druid on `ip_address`): Approximate count of unique IP addresses.
                - `fraudulent_click_count` (longSum, filtered aggregator where `is_fraudulent` is true): Count of clicks marked as fraudulent.

        #### 6.2.2. Example: Minute-by-Minute Aggregation Table (`ad_clicks_1minute`)
            - Similar structure to hourly, but `__time` represents the start of the minute window.
            - Dimensions and metrics would be the same.
            - Typically has a shorter retention period than hourly/daily aggregates.

        #### 6.2.3. Metadata Tables (in RDBMS like PostgreSQL)
            - **`campaigns` Table**:
                - `campaign_id` (PK, int/string)
                - `campaign_name` (string)
                - `advertiser_id` (FK, int/string)
                - `start_date` (date)
                - `end_date` (date)
                - `budget` (decimal)
                - ... other campaign attributes
            - **`ads` Table**:
                - `ad_id` (PK, int/string)
                - `ad_name` (string)
                - `campaign_id` (FK, int/string)
                - `creative_url` (string)
                - `target_url` (string)
                - ... other ad attributes
            - **`advertisers` Table**:
                - `advertiser_id` (PK, int/string)
                - `advertiser_name` (string)
                - ... other advertiser attributes
            - **`publishers` Table**:
                - `publisher_id` (PK, int/string)
                - `publisher_name` (string)
                - `website_url` (string)
                - ... other publisher attributes

        - **Note on Cardinality**: Dimensions like `user_id` or raw `ip_address` are usually not stored directly as dimensions in OLAP systems for aggregated views due to high cardinality, which can impact performance and storage. Instead, unique counts are derived using HyperLogLog (HLL) or similar sketches. If exact unique user lists are needed for specific queries, those might be handled differently, perhaps by querying the raw data store.

## 7. Scalability & Performance
    - **Overall Strategy**: Design for horizontal scalability across all layers. Utilize cloud-native services and managed platforms where possible to simplify scaling operations.
    ### 7.1. Ingestion Layer Scalability
        - **Stateless Services**: Click Ingestion Services are designed to be stateless, allowing them to be easily scaled out horizontally behind a load balancer (e.g., AWS ALB/NLB, Nginx).
        - **Auto-scaling**: Implement auto-scaling groups based on metrics like CPU utilization, request count, or queue length in Kafka.
        - **Global Distribution**: Deploy ingestion endpoints in multiple geographic regions closer to users to reduce latency (e.g., using AWS Global Accelerator or Cloudflare). Each region can publish to a local Kafka cluster or a global one.

    ### 7.2. Message Queue (Kafka) Scalability
        - **Partitioning**: Increase the number of partitions for Kafka topics (`raw_clicks`, `aggregated_clicks`) to increase parallelism for both producers (ingestion service) and consumers (Flink).
        - **Broker Scaling**: Scale the Kafka cluster by adding more brokers to handle increased load and storage.
        - **Topic Design**: Use appropriate partition keys (e.g., `ad_id`, `user_id` if balanced, or round-robin) to ensure even data distribution across partitions.

    ### 7.3. Stream Processing (Flink) Scalability
        - **Parallelism**: Adjust the parallelism of Flink operators (sources, mappers, keyBy, window operators, sinks) to match the number of Kafka partitions and available processing resources (TaskManagers and slots).
        - **Resource Allocation**: Scale Flink clusters by adding more TaskManagers. Utilize YARN, Kubernetes, or Flink's standalone cluster manager.
        - **State Backend**: Choose a scalable state backend for Flink (e.g., RocksDB on HDFS/S3) to handle large state sizes for aggregations and windowing.
        - **Efficient Serialization**: Use efficient serialization formats like Avro or Kryo within Flink.
        - **Backpressure Handling**: Flink has built-in backpressure mechanisms. Monitor and adjust to prevent system overload.

    ### 7.4. Data Storage Scalability
        - **Raw Click Storage (S3/GCS)**: Object storage scales virtually infinitely. Proper partitioning (by date) ensures query performance for batch jobs.
        - **Aggregated Data Storage (e.g., Druid/ClickHouse)**:
            - **Druid**: Scales by adding different node types (Historical, MiddleManager, Broker, etc.). Data is sharded by time and optionally by other dimensions.
            - **ClickHouse**: Scales by adding more nodes to the cluster and sharding/replicating data.
            - Both support distributed queries.
        - **Metadata Storage (RDBMS)**:
            - Use read replicas for read-heavy workloads.
            - Consider sharding if write load becomes a bottleneck, though typically metadata changes less frequently.
            - Connection pooling at the application layer.

    ### 7.5. Reporting & Querying API Scalability
        - **Stateless API Services**: The Query Service Layer should be stateless and scaled horizontally.
        - **Caching**:
            - **API Gateway Caching**: Cache common API responses at the API Gateway level.
            - **Query Service Caching**: Implement a distributed cache (e.g., Redis, Memcached) in the Query Service to cache results from the OLAP database for frequently requested reports.
            - **Client-Side Caching**: Encourage clients to use HTTP caching headers.
        - **Database Connection Pooling**: Efficiently manage connections to the OLAP database.

    ### 7.6. Performance Considerations & Benchmarks
        - **Low Latency Ingestion**: Aim for <50ms for the p99 latency from click reception to Kafka acknowledgement.
        - **Near Real-time Aggregation**: Aggregation latency (click in Kafka to visible in OLAP store) should be within 1-5 minutes.
        - **Fast Reporting Queries**: P95 query latency for common reports should be <1-2 seconds. Complex ad-hoc queries might take longer.
        - **Throughput**: Design for peak ingestion (e.g., 10k-100k CPS) and query (e.g., 500-1000 QPS) loads as per NFRs.
        - **Benchmarking & Load Testing**:
            - Regularly perform load tests simulating peak traffic to identify bottlenecks.
            - Use tools like k6, JMeter, or custom scripts.
            - Monitor key performance indicators (KPIs) like latency, throughput, error rates, and resource utilization (CPU, memory, network, I/O) across all components.
        - **Data Partitioning & Indexing**: Critical for OLAP query performance. Ensure appropriate partitioning, sorting, and indexing strategies are in place in Druid/ClickHouse.
        - **Query Optimization**: Analyze and optimize slow queries in the OLAP database. Use query execution plans.

## 8. Reliability & Fault Tolerance
    - **Goal**: Achieve high availability (e.g., 99.99% for ingestion, 99.9% for reporting) and prevent data loss.
    ### 8.1. Component-Level Redundancy
        - **Ingestion Service**: Run multiple instances behind a load balancer across different Availability Zones (AZs).
        - **Message Queue (Kafka)**:
            - **Replication**: Configure topic replication factor (e.g., 3) across different brokers in different AZs.
            - **ISR (In-Sync Replicas)**: Set `min.insync.replicas` (e.g., 2) to ensure writes are acknowledged only after being written to a minimum number of replicas.
            - **Broker Redundancy**: Ensure the Kafka cluster can tolerate broker failures (e.g., N-1 or N-2 broker failures).
        - **Stream Processing (Flink)**:
            - **JobManager High Availability**: Configure Flink JobManager HA (e.g., using ZooKeeper).
            - **TaskManager Failover**: TaskManagers are stateless for computation; tasks can be restarted on other TaskManagers.
            - **State Checkpointing**: Regularly checkpoint Flink operator state to a distributed, durable file system (e.g., S3, HDFS). Configure appropriate checkpointing intervals and retention.
            - **Savepoints**: Take periodic savepoints for planned maintenance or version upgrades.
        - **Data Storage**:
            - **Raw Click Storage (S3/GCS)**: These services offer built-in high durability and availability (e.g., 99.999999999% durability for S3). Use cross-region replication if DR across regions is required.
            - **Aggregated Data Storage (Druid/ClickHouse)**:
                - **Druid**: Replicate segments across Historical nodes. Ensure critical services (Coordinator, Overlord, Broker) have HA setups. Deep storage (S3/HDFS) for segments provides durability.
                - **ClickHouse**: Use data replication and sharding across multiple nodes and AZs.
            - **Metadata Storage (RDBMS)**: Use managed database services with multi-AZ deployment options (e.g., AWS Aurora, RDS Multi-AZ). Regular backups and point-in-time recovery.
        - **Reporting & Query API**: Run multiple instances behind a load balancer across different AZs.

    ### 8.2. Data Guarantees & Consistency
        - **Exactly-Once Semantics (EOS)**:
            - Aim for EOS for the core pipeline from Kafka into Flink and from Flink to the aggregated data store.
            - Flink supports EOS with specific Kafka connectors (transactional producers) and sinks that support transactions or idempotent writes (e.g., two-phase commit sinks).
        - **Data Validation & Error Handling**:
            - Implement robust validation at the ingestion service and within Flink.
            - Invalid/corrupt messages should be routed to a Dead Letter Queue (DLQ) or a separate error stream for investigation, rather than crashing the pipeline.
        - **Reprocessing**:
            - The ability to reprocess raw data (from Kafka if retention allows, or from Raw Click Storage) is crucial for correcting errors or applying new logic. Flink jobs can be re-run from savepoints or by re-reading source data.

    ### 8.3. Disaster Recovery (DR)
        - **Regional Failover Strategy**:
            - For critical components, consider a multi-region active/passive or active/active setup, depending on RTO/RPO requirements and cost.
            - This typically involves replicating data (Kafka topics, database backups, Druid segments in deep storage) to a secondary region.
        - **Backup and Restore Procedures**:
            - Regularly back up all persistent state and configurations (Flink savepoints, database backups, Kafka configurations).
            - Test restore procedures periodically.
        - **RTO (Recovery Time Objective) & RPO (Recovery Point Objective)**: Define RTO and RPO for different parts of the system and design DR strategies accordingly.

    ### 8.4. Failure Detection & Alerting
        - **Monitoring**: Comprehensive monitoring of all components (covered in Section 10).
        - **Health Checks**: Implement health check endpoints for all services. Load balancers use these to route traffic away from unhealthy instances.
        - **Automated Recovery**: Where possible, automate recovery processes (e.g., restarting failed Flink tasks, auto-scaling groups replacing unhealthy instances).

## 9. Security Considerations
    - **Principle of Least Privilege**: Ensure components and users only have access to the resources and data necessary for their functions.
    ### 9.1. Data Encryption
        - **In Transit**:
            - All external communication (client to ingestion service, API calls) MUST use HTTPS/TLS.
            - Internal communication between services (e.g., Flink to Kafka, Flink to Druid, API service to Druid) should also be encrypted using TLS where supported by the components.
        - **At Rest**:
            - **Raw Click Storage (S3/GCS)**: Enable server-side encryption (e.g., SSE-S3, SSE-KMS).
            - **Aggregated Data Storage (Druid/ClickHouse)**: Enable encryption at rest for data files and backups. Druid segments in deep storage (S3) will inherit S3's encryption.
            - **Message Queue (Kafka)**: Kafka supports TLS for broker-client and inter-broker communication. Data on disk can be encrypted by the underlying OS/filesystem or via Kafka's disk encryption features if available/needed.
            - **Metadata Storage (RDBMS)**: Use transparent data encryption (TDE) or filesystem-level encryption.
            - **Secrets Management**: Use a dedicated secrets management service (e.g., HashiCorp Vault, AWS Secrets Manager, Azure Key Vault) for API keys, database credentials, and TLS certificates.

    ### 9.2. Access Control & Authentication
        - **Ingestion Endpoints**:
            - Consider API key authentication for POST-based ingestion if from trusted partners.
            - Pixel tracking (GET) is harder to authenticate directly but relies on WAF/rate limiting.
        - **Reporting API**:
            - Implement strong authentication (e.g., OAuth 2.0, JWT tokens, API Keys for service accounts).
            - Role-Based Access Control (RBAC) to restrict access to specific reports or data based on user roles/permissions.
        - **Internal Services**:
            - Use mutual TLS (mTLS) or service mesh authentication (e.g., Istio, Linkerd) for inter-service communication if feasible.
            - Network policies (e.g., Kubernetes Network Policies, Security Groups) to restrict communication paths between components.
        - **Infrastructure Access**:
            - Secure SSH access to servers (key-based, bastion hosts).
            - IAM roles and policies for cloud resources.

    ### 9.3. Network Security
        - **Firewalls/Security Groups**: Restrict inbound and outbound traffic to only necessary ports and IP ranges for each component.
        - **VPC (Virtual Private Cloud)**: Deploy the system within a VPC. Use private subnets for backend components (Kafka, Flink, databases) and public subnets only for internet-facing load balancers and API gateways.
        - **WAF (Web Application Firewall)**: Deploy a WAF in front of ingestion endpoints and reporting APIs to protect against common web exploits (SQL injection, XSS, etc.) and to help with DDoS mitigation.
        - **Rate Limiting**:
            - At the API Gateway/Load Balancer for ingestion and reporting APIs to prevent abuse.
            - Potentially within Flink for specific heavy operations if needed.

    ### 9.4. Application Security
        - **Input Validation**: Rigorously validate all incoming data at the ingestion service and before processing by Flink to prevent injection attacks and malformed data issues.
        - **Dependency Management**: Regularly scan and update third-party libraries to patch known vulnerabilities.
        - **Secure Coding Practices**: Follow secure coding guidelines (e.g., OWASP Top 10).
        - **Regular Security Audits & Penetration Testing**: Conduct periodic security assessments.

    ### 9.5. Data Privacy & Compliance
        - **PII Handling**:
            - Identify and classify PII (e.g., `user_id`, `ip_address`).
            - Implement anonymization or pseudonymization techniques as early as possible in the pipeline if full PII is not required for downstream processing or aggregation (e.g., hash `user_id`, truncate/anonymize last octet of IP after geo-lookup).
            - Adhere to relevant data privacy regulations (e.g., GDPR, CCPA).
        - **Audit Logs**: Maintain audit logs for sensitive operations, access to data, and administrative actions.

## 10. Monitoring & Alerting
    - **Goals**: Ensure system health, detect and diagnose issues quickly, provide insights into performance and usage patterns, and trigger automated or manual responses to incidents.
    ### 10.1. Key Metrics to Monitor
        - **Overall System Health**:
            - End-to-end click processing latency (from ingestion to availability in reporting).
            - Overall click throughput.
            - System-wide error rates.
        - **10.1.1. Ingestion Service**:
            - Request rate (per endpoint).
            - Request latency (average, p95, p99).
            - Error rates (HTTP 4xx, 5xx).
            - Resource utilization (CPU, memory, network) of service instances.
            - Number of active connections.
            - Rate of messages published to Kafka.
        - **10.1.2. Message Queue (Kafka)**:
            - Topic-level metrics: message rates (in/out), byte rates (in/out), number of messages in backlog (consumer lag per partition).
            - Broker-level metrics: CPU, memory, disk I/O, network I/O, disk space utilization, under-replicated partitions, offline partitions.
            - Producer/Consumer metrics: send/receive rates, error rates, latency.
            - ZooKeeper health (if applicable).
        - **10.1.3. Stream Processing (Flink)**:
            - Job health: uptime, number of restarts, last checkpoint status, checkpoint duration, checkpoint size.
            - Latency: end-to-end latency per record, operator-level processing time.
            - Throughput: records/bytes processed per second (input and output of jobs/operators).
            - Watermarks: current event time watermarks to monitor data freshness and late data.
            - Resource utilization: TaskManager CPU, memory, network, JVM heap usage, garbage collection stats.
            - Kafka consumer lag for Flink sources.
            - Errors and exceptions within Flink jobs.
            - State size (for stateful operations).
        - **10.1.4. Aggregated Data Storage (e.g., Druid/ClickHouse)**:
            - Query latency (average, p95, p99).
            - Query throughput (queries per second).
            - Query error rates.
            - Ingestion rate and latency (for real-time nodes if applicable, e.g., Druid MiddleManagers).
            - Storage utilization, disk space.
            - Node health: CPU, memory, network, I/O for all node types (e.g., Druid Historicals, Brokers, Coordinators; ClickHouse nodes).
            - Segment/Part availability and count (Druid).
        - **10.1.5. Reporting & Query API**:
            - API request rate.
            - API request latency (average, p95, p99).
            - API error rates (HTTP 4xx, 5xx).
            - Resource utilization (CPU, memory, network) of API service instances.
            - Cache hit/miss rates (if caching is implemented).
        - **10.1.6. Infrastructure & Common Services**:
            - Load balancer metrics: active connections, request counts, error rates, health check status.
            - Database (Metadata Store) metrics: query latency, connection count, CPU/memory/storage utilization.
            - Network throughput and latency between components/AZs.

    ### 10.2. Logging Strategy
        - **Structured Logging**: Use structured log formats (e.g., JSON) for all services to facilitate easier parsing, searching, and analysis. Include contextual information like request IDs, user IDs (anonymized if PII), component name, and timestamps.
        - **Centralized Logging**: Ship logs from all components to a centralized logging system (e.g., ELK Stack - Elasticsearch, Logstash, Kibana; Splunk; Grafana Loki; AWS CloudWatch Logs).
        - **Log Levels**: Implement appropriate log levels (DEBUG, INFO, WARN, ERROR, CRITICAL) and allow dynamic adjustment of log levels for troubleshooting.
        - **Correlation IDs**: Propagate a unique correlation ID across service calls to trace a single request's journey through the system.
        - **Key Information to Log**:
            - Errors and exceptions with stack traces.
            - Significant lifecycle events (service start/stop, job submission/completion).
            - Important business events (e.g., a click received, an aggregation window completed).
            - Security-relevant events (authentication failures, authorization denials - see Audit Logs in Security).

    ### 10.3. Alerting Strategy
        - **Alerting Tools**: Integrate monitoring systems with alerting platforms (e.g., Prometheus Alertmanager, Grafana Alerting, PagerDuty, Opsgenie, AWS SNS).
        - **Alert Severity Levels**: Define severity levels for alerts (e.g., P1/Critical, P2/Error, P3/Warning) to guide response urgency.
        - **On-Call Rotations**: Establish on-call rotations for critical alerts that require immediate attention.
        - **Actionable Alerts**: Ensure alerts are actionable and provide sufficient context for troubleshooting. Avoid noisy alerts.
        - **Key Alerts to Configure**:
            - **System Down/Unavailable**: Any critical component unresponsive (ingestion endpoints, Kafka brokers, Flink JobManager, OLAP DB, Reporting API).
            - **High Error Rates**: Sustained high error rates in any component.
            - **High Latency**: Significant increase in processing or query latency.
            - **Resource Exhaustion**: Critical resource utilization (CPU, memory, disk space) nearing capacity.
            - **Data Pipeline Issues**:
                - High Kafka consumer lag (Flink falling behind).
                - Flink job failures or frequent restarts.
                - Failed Flink checkpoints.
                - OLAP DB ingestion failures.
            - **Security Alerts**: (e.g., from WAF, intrusion detection systems).
            - **DLQ Volume**: Significant increase in messages going to Dead Letter Queues.

    ### 10.4. Dashboards
        - **Purpose**: Provide visual overviews of system health, performance, and key business metrics.
        - **Tools**: Use dashboarding tools like Grafana, Kibana, AWS CloudWatch Dashboards, Apache Superset (for business metrics from OLAP).
        - **Types of Dashboards**:
            - **High-Level Overview**: Overall system status, key KPIs.
            - **Component-Specific Dashboards**: Detailed metrics for each service (Kafka, Flink, API, etc.).
            - **Business Metrics Dashboards**: Click trends, top campaigns, geo distributions, etc. (queried from the OLAP store).

## 11. Trade-offs
    - **Overall Philosophy**: The design prioritizes scalability, near real-time processing, and reliability, which often involves choosing more complex, distributed systems and managed services where appropriate to balance operational overhead.

    ### 11.1. Real-time vs. Batch Processing
        - **Decision**: Primarily a real-time stream processing pipeline (Flink) for aggregations, with optional batch processing on raw stored data for reprocessing, ML, or complex analytics.
        - **Trade-off**:
            - **Pros (Real-time)**: Faster insights, ability to react quickly to trends, fresher data for reporting.
            - **Cons (Real-time)**: More complex to design and manage, potentially higher operational cost, handling late data and exactly-once semantics can be challenging.
            - **Batch Alternative**: Simpler to implement, potentially lower cost for some workloads, but higher latency for insights.
        - **Justification**: The business requirement for near real-time reporting and aggregation (1-5 min latency) drives the choice of stream processing.

    ### 11.2. Cost vs. Performance vs. Complexity
        - **Decision**: Leaning towards higher performance and scalability, which can increase cost and complexity (e.g., using Kafka, Flink, Druid/ClickHouse).
        - **Trade-off**:
            - **Higher Performance/Scalability**: Meets demanding NFRs (high throughput, low latency).
            - **Increased Cost**: Licensing (if any), infrastructure for distributed systems, specialized expertise needed.
            - **Increased Complexity**: More moving parts, harder to debug, steeper learning curve for the team.
        - **Mitigation**: Use managed cloud services (e.g., managed Kafka, managed Kubernetes for Flink, managed OLAP DBs if available and suitable) to reduce operational complexity, though this might shift costs. Start with appropriately sized clusters and scale as needed.

    ### 11.3. Choice of Specific Technologies
        - **Kafka vs. other message queues (e.g., Pulsar, RabbitMQ)**:
            - **Decision**: Kafka.
            - **Trade-off**: Kafka offers excellent throughput, scalability, and a strong ecosystem (especially with Flink). Pulsar is a strong alternative with built-in geo-replication and tiered storage but might have a smaller community or fewer managed service options. RabbitMQ is generally simpler but might not scale to the same levels for this specific high-throughput use case.
        - **Flink vs. Spark Streaming (or other stream processors)**:
            - **Decision**: Flink.
            - **Trade-off**: Flink provides robust event-time processing, exactly-once semantics, and fine-grained control over state management, making it well-suited for complex aggregations. Spark Streaming (especially with Structured Streaming) is also capable but Flink is often favored for pure streaming applications with lower latency requirements.
        - **Druid/ClickHouse vs. other OLAP/TSDBs (e.g., TimescaleDB, InfluxDB, Snowflake)**:
            - **Decision**: Lean towards Druid or ClickHouse.
            - **Trade-off**: Druid/ClickHouse are optimized for real-time ingestion and fast OLAP queries on large datasets, fitting the ad aggregation use case well.
                - **TSDBs (InfluxDB, TimescaleDB)**: Might be simpler for pure time-series analysis but less flexible for high-cardinality, multi-dimensional slicing and dicing.
                - **Cloud Data Warehouses (Snowflake, BigQuery)**: Offer excellent scalability and ease of use (fully managed) but might have higher latency for real-time ingestion or per-query cost considerations for very high query volumes compared to self-managed Druid/ClickHouse.
        - **Justification**: Technology choices are based on their ability to meet the specific NFRs (throughput, latency, scalability, EOS) and the strength of their integration capabilities.

    ### 11.4. Data Consistency vs. Availability vs. Latency
        - **Ingestion Path**:
            - **Click Ingestion to Kafka**: Aims for high availability and low latency. Producer acks (`acks=all` with `min.insync.replicas=2`) provide strong durability.
        - **Aggregation Path**:
            - **Flink Processing**: Aims for exactly-once semantics (strong consistency for processed data). Checkpointing ensures fault tolerance.
            - **Aggregated Store**: Eventual consistency is often acceptable for aggregated reporting data (e.g., data might take a few minutes to appear after a click). Stronger consistency might increase latency or reduce availability.
        - **Reporting Path**:
            - Prioritizes availability and low query latency for reports. Data might be slightly stale (minutes).
        - **Trade-off**: Achieving strong consistency across a distributed real-time system can be complex and impact latency/availability. The design accepts eventual consistency for aggregated views to maintain high throughput and low query latency. Raw data path maintains stronger consistency.

    ### 11.5. Build vs. Buy (Managed Services)
        - **Decision**: Favor managed services where they meet requirements and offer operational benefits, even if per-unit cost seems higher.
        - **Trade-off**:
            - **Buy (Managed Services)**: Reduced operational overhead, faster deployment, built-in HA/scalability features. Can lead to vendor lock-in and potentially higher direct costs.
            - **Build (Self-Managed)**: More control, potentially lower direct infrastructure costs (if expertise is available). Higher operational burden, requires significant in-house expertise.
        - **Examples**: Managed Kafka (e.g., Confluent Cloud, AWS MSK), managed Kubernetes (EKS, GKE), managed databases (RDS, Aurora), managed OLAP (if chosen, e.g. Redshift, BigQuery, or managed ClickHouse/Druid offerings).

    ### 11.6. Deduplication Strategy
        - **Decision**: Short-term window deduplication in Flink based on a composite key.
        - **Trade-off**:
            - **Pros**: Catches most duplicates occurring due to client-side retries or network issues. Relatively simple to implement in a stream processor.
            - **Cons**: Might miss duplicates outside the time/state window. Perfect deduplication over long periods is very hard and resource-intensive in a high-throughput streaming system.
        - **Justification**: Balances effectiveness with complexity and resource cost. For financial reconciliation, longer-term batch deduplication might be needed on raw data.

## 12. Future Considerations / Enhancements
    - This section outlines potential future improvements and expansions for the Ad Click Aggregator system.

    ### 12.1. Advanced Fraud Detection
        - **Machine Learning Models**: Develop and deploy more sophisticated ML models (e.g., supervised, unsupervised, deep learning) using the raw click data to identify complex fraud patterns beyond basic rules. This could include behavioral analysis, botnet detection, and click-farm identification.
        - **Feedback Loop**: Implement a feedback loop where fraud scores or confirmed fraudulent activities are fed back into the models for continuous learning and improvement.
        - **External Threat Intelligence**: Integrate with external threat intelligence feeds for up-to-date IP blacklists, known malicious user agents, etc.

    ### 12.2. Real-Time Bidding (RTB) Integration & Feedback
        - **Performance Feedback**: Provide near real-time aggregated performance data (click-through rates, conversion signals if available) back to RTB platforms to optimize bidding strategies.
        - **Spend Pacing Insights**: Offer insights into ad spend velocity and budget consumption based on click volumes and associated costs.

    ### 12.3. Deeper User Behavior Analysis
        - **User Journey/Path Analysis**: Analyze sequences of clicks and interactions (if other interaction data is available) to understand user journeys leading to conversions or other desired outcomes.
        - **Sessionization**: Implement more advanced sessionization logic to group user activities for better contextual analysis.
        - **Audience Segmentation**: Use aggregated click patterns and enriched data to identify and define audience segments for targeted advertising or content.

    ### 12.4. Cross-Device Attribution
        - **Probabilistic & Deterministic Matching**: Explore techniques for cross-device attribution by integrating with identity resolution providers or developing in-house methods to link user activity across multiple devices.
        - **Attribution Modeling**: Support various attribution models (e.g., first-touch, last-touch, linear, time-decay) to help advertisers understand the impact of different touchpoints.

    ### 12.5. A/B Testing Framework Support
        - **Data Provisioning**: Provide clean, aggregated data segmented by A/B test variations to allow for robust statistical analysis of ad creative, campaign, or landing page performance.
        - **Automated Reporting**: Develop automated reports specifically for A/B test results.

    ### 12.6. Enhanced Self-Service Capabilities for Clients
        - **Custom Reporting**: Allow advertisers/publishers to build and schedule their own custom reports through an enhanced UI or API.
        - **Custom Alerting**: Enable clients to set up alerts based on specific metrics or thresholds relevant to their campaigns (e.g., sudden drop in CTR, budget nearing limit).

    ### 12.7. Cost Optimization and Efficiency
        - **Continuous Review**: Regularly review resource utilization, storage costs, and processing efficiency to identify optimization opportunities (e.g., right-sizing instances, optimizing Flink jobs, data lifecycle policies for raw storage).
        - **Spot Instances/Reserved Instances**: Strategically use cost-saving options like spot instances for stateless workloads or reserved instances for baseline capacity.

    ### 12.8. Integration with Other MarTech/AdTech Platforms
        - **Data Management Platforms (DMPs)**: Integrate with DMPs for audience data enrichment or segmentation.
        - **Customer Relationship Management (CRMs)**: Potentially link click data with customer data in CRMs for a more holistic view (requires careful PII handling and consent).

## 13. Conclusion
    - (Summary of the design, reiteration of how it meets key requirements, and final thoughts.)

## 14. Appendix (Optional)
    - (Detailed diagrams, capacity estimation worksheets, specific configurations, glossaries, etc.)
